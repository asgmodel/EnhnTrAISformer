======= Directory to store trained models: ./results/ct_dma-pos-pos_vicinity-10-40-blur-True-False-2-1.0-data_size-100-100-30-360-embd_size-256-256-128-128-head-8-8-bs-32-lr-0.0006-seqlen-18-120/
2025-03-26 00:34:04,227 - models - number of parameters: 5.735092e+07
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
epoch 1 iter 287: loss 4.17115. lr 5.993227e-04: 100% 288/288 [02:02<00:00,  2.35it/s]
2025-03-26 00:36:08,417 - root - Training, epoch 1, loss 8.10189, lr 5.993227e-04.
2025-03-26 00:36:14,628 - root - Valid, epoch 1, loss 4.04761.
2025-03-26 00:36:14,630 - root - Best epoch: 001, saving model to ./results/ct_dma-pos-pos_vicinity-10-40-blur-True-False-2-1.0-data_size-100-100-30-360-embd_size-256-256-128-128-head-8-8-bs-32-lr-0.0006-seqlen-18-120/model.pt
2025-03-26 00:36:17,805 - numexpr.utils - NumExpr defaulting to 2 threads.
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
epoch 2 iter 125: loss 2.98032. lr 6.000000e-05:  44% 126/288 [00:55<01:10,  2.28it/s]
Traceback (most recent call last):
  File "/content/EnhnTrAISformer/main_training.py", line 106, in <module>
    main()
  File "/content/EnhnTrAISformer/main_training.py", line 40, in main
    trainer.train()
  File "/content/EnhnTrAISformer/utils_training.py", line 180, in train
    run_epoch('Training', epoch=epoch)
  File "/content/EnhnTrAISformer/utils_training.py", line 126, in run_epoch
    progress = float(self.tokens - config.warmup_tokens) / float(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
